---
topic: Machine Learning
type: Technical
tags:
  - + Centre for AI & Skills
title: >
  Hardware-Algorithm Co-Design for ML Inference
speaker: Alessandro Pierro
institution: Intel & LMU Munich
webpage: https://alessandropierro.eu/ 
date: 2025-10-29 12:30
venue: Moore Annex 034B
link: https://teams.microsoft.com/l/meetup-join/19%3ameeting_NGRhZjQ1OGMtNzA4Mi00MWMxLWE1ODEtNzJjNmEwZjBkZTg1%40thread.v2/0?context=%7b%22Tid%22%3a%222efd699a-1922-4e69-b601-108008d28a2e%22%2c%22Oid%22%3a%229c6a898b-86f6-4344-8f36-27129ffe3748%22%7d 
recording: https://rhul-my.sharepoint.com/:v:/g/personal/anand_subramoney_rhul_ac_uk/EUf5vtlKCQFFpZfn7SBRH2EB_hN1aQySjadNbbasaViIQA?e=Dd2Ei4&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D 
bio: >
  Alessandro Pierro is a researcher at Intel Labs and a doctoral candidate in computer science at the Ludwig-Maximilians-Universität München. His research focuses on accelerating machine learning and mathematical optimization workloads through hardware-algorithm co-design, using advances in parallel computing architectures. He is currently working on energy-efficient inference for linear recurrent networks to enable sequence modeling at the edge.

abstract: >
  The growing demand for AI accelerators presents an opportunity to validate the technological readiness of emerging hardware architectures.
  This requires understanding how alternative computational paradigms perform on real applications and identifying which algorithmic trends align with their inherent strengths. This seminar will provide empirical results on current ML workloads running on the Intel Loihi 2 accelerator, a sparse, event-driven, spatially-mapped system. Results on State Space Models and recurrent LLMs demonstrate where Loihi 2 can excel, as well as which architecture-level enhancements could improve its performance on these workloads. We will also cover the implications for hardware architectures arising from recent trends in large-scale ML workloads, including sparse mixtures-of-experts, recurrent reasoning, speculative decoding, and hierarchical networks.
---
